<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
          content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VTC-Net</title>
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
  
      function gtag() {
        dataLayer.push(arguments);
      }
  
      gtag('js', new Date());
  
      gtag('config', 'G-PYVRSFMDRL');
    </script>
  
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
  
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <style>
      body {
          margin: 0;
          padding: 10px; /* 页面两边的间距 */
          font-family: Arial, sans-serif;
      }

      section {
          margin-bottom: 10px; /* 每个section之间的间距 */
      }

      /* Section 1 样式 */
      .section1 {
          background-color: #f0f8ff;
          padding: 5px;
          /* border-left: 5px solid #007BFF; */
          font-size: 20px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
      }

      /* Section 2 样式 */
      .section2 {
          /* background-color: #f8d7da; */
          padding: 10px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #d00606;
          margin-top: 3px;
      }

      /* Section 3 样式 */
      .section3 {
          /* background-color: #f8d7da; */
          padding: 1px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #220202;
      }
      .sectionab {
      /* 你可以根据需要调整section的外边距 */
      margin: 5px;
    }
 
    .publication-abs {
      text-align: justify; /* 设置文本两端对齐 */
      font-size: 20px; /* 设置字体大小 */
      padding: 0 10px; 
      font-weight: bold
      /* 如果需要上下内边距，可以添加类似下面的行 */
      /* margin-top: 10px; */
      /* margin-bottom: 10px; */
    }

 
    .publication-text {
      text-align: justify; /* 设置文本两端对齐 */
      font-size: 20px; /* 设置字体大小 */
      padding: 1px 40px; /* 增加上下和左右的内边距，左右设置为40px以增加两端距离 */
      font-weight: bold; /* 字体加粗 */
      /* 如果需要上下外边距，可以添加类似下面的行 */
      /* margin-top: 10px; */
      /* margin-bottom: 10px; */
      line-height: 1.6; /* 可选：增加行高以提高可读性 */
    }
/* 
      .img {
              margin-top: 1px; 
            }
          
            .img img {
              max-width: 60%; 
              height: auto;
            }
          
            .img .content {
              margin-top: 3px; 
              font-size: 5px;
              text-align: center;
              line-height: 1.6; 
            } */
    .img img {
        max-width: 50%;
        /* width: 1200px;  */
        height: auto; /* 保持图片宽高比 */
      }
 
  /* 设置当前 section 与上一个 section 的间距 */
      .img {
        margin-top: 5px; /* 调整间距大小，例如 20px */
      }
  </style>
  </head>
<body>
  <section class="section1">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- 标题 -->
            <h1 class="title is-1 publication-title">An Embodied Vision-Tactile-Based Single-view Shape Completion method in 3D scence </h1>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section2">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Jiarui Hu, </span>
              <span class="author-block">Feng Luan, </span>
              <span class="author-block">Changshi Zhou, </span>
              <span class="author-block">Zheng Yan, </span>
              <span class="author-block">Zhipeng Wang, </span>
              <span class="author-block">Yanmin Zhou, </span>
              <span class="author-block">Jiguang Yue, </span>
              <span class="author-block">Bin He</span>
            </div>
   
            <div class="is-size-5 publication-authors">
              <span class="author-block">Tongji University</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="sectionab">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-abs">
              Abstract: three-dimensional(3D) perception is essential for embodied perception systems to accurately understand their environment and perform elaborate manipulation tasks. 
              Visual sensors in robot often cannot capture complete geometric information due to a single viewpoint. Point cloud completion methods are often used to recover the shape of an object from
              a local point cloud. However, most completion methods fail for the recovery of object details by single visual modality. 
              In this work, we propose a two-stage visual-tactile fusion network for high-precision object completion. 
              Vision sensors are used to capture global features of an object. 
              As a crucial component of robotic perception, tactile sensing provides robots with more detailed local object characteristics. 
              This framework effectively utilizes the spatial feature between sample instances in three dimension to improve the reduced characterization capability of object shape completion tasks. 
              In our model,VTC-Net, conceptual awareness of object categories is used for guiding point cloud generation to improve the cognitive capacity of the network.
              This design enables the network to effectively extract common features among objects of the same category while enhancinginter-class discriminability. 
              Furthermore, a refine channel is designed to solve point cloud distortion in real scenes. 
              Compared to other methods, ours is more convenient for applying real-worldscenarios. 
              Extensive experiments validate the superiority of the proposed method compared to other state-of-the-art methods onthree datasets. 
              In addition, we also build an embodied perceptual multi-modal dataset to better apply the proposed method to robot manipulation tasks in real-world scenarios
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/introduce.png" alt="Centered Image" style="width: 60%; height: auto;"/> 
    </div>
  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 1:  Task data presentation. The true values are obtained by scanning  real objects by a handheld device. Because the data acquired by the depth camera is distorted, we also adjusted the observations based on the introduction of tactile data. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              Introduction:In this paper, inspired by human cognitive behavior, we build a multi-modal perception platform for modeling robotic embodied perception systems. 
              Humans make extensive use of visual and tactile perception in performing grasping. For example, when a person wants to hold a mug, he may not be able to see the handle of the mug due to the limitation of the viewing angle. 
              At this point, his subconscious knows what category the object to be held is in. Then he reaches out and touches or explores the handle area on the back of the mug, thus picking up it easily. Specifically, our approach contains two channels: the visual and the tactile channels.
              The platform contains a visual-tactile data collection approach and a concept-aware visual-tactile fusion completion method as shown in Fig.\ref{figure_liucheng}.
              For the visual channel, a depth camera simulates the visual input device of an embodied robot to capture single-view information of objects. 
              For the tactile sensing modality, a custom-designed tactile sensor mounted on the robotic arm's end-effector simulates embodied robotic hand-object interaction to acquire contralateral information from a single viewpoint. 
              Then, we use VTC-Net for feature fusion and predict the object class to guide a pyramid decoder generating multi-resolution missing point clouds. Further, we find that point clouds acquired by cameras are usually deviated, due to camera itself and object material. For this reason, we designed an aberration refine method for the captured point cloud in order to enhance the effectiveness of completion.

              To summarize, our main contributions are:<br><br>
              <strong>1. A multi-modal perception platform is built for modeling robotic embodied perception systems and collect a multi-modal embodied perception dataset, VisTacPoint, for robotic spatial intelligence. To the best of our knowledge, this is the first large-scale integrated embodied perception-based visual-tactile fusion dataset from the real world.</strong><br><br>
              <strong>2. A refine-concept-aware point cloud completion framework is proposed. The VTC-Net as the main component of the framework use object categories to guide point cloud generation to improve prediction accuracy.</strong><br><br>
              <strong>3. We also have conducted extensive experiments on ShapeNet, ModelNet40 and VisTacPoint to demonstrate the effectiveness on our model.</strong>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="sectionfig1text">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              Multi-modal learning is a key area in Artificial Intelligence (AI) research, focusing on integrating information from diverse sources, 
              such as vision, sound, text and touch, to enhance machine learning models [18][19][20]. Recently, this field has witnessed significant 
              advancements and broad applications. This paper specifically explores fusion techniques for visual and tactile data. 
              Visual-tactile fusion is crucial for enabling robotic systems to achieve comprehensive object understanding and dexterous manipulation, 
              mirroring human-like interaction capabilities. Visual sensors can capture high-resolution images, enabling robots to accurately identify 
              the position and shape of objects [21][22]. Tactile sensors, on the other hand, can detect physical properties such as hardness and 
              texture [23][24]. Combining these two types of data can improve recognition accuracy, particularly for objects that appear similar but 
              differ in physical characteristics. The visual-tactile multi-modal learning process and its applications are illustrated in Fig. 1. The 
              multi-modal perception model acts as the brain of a robot, providing the cognitive foundation for subsequent tasks, such as object 
              understanding and robotic grasping, by processing data from its sensing devices.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/FRAMEWORK.png" alt="Centered Image"/>  
    </div>
  </section>

  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig2.  Visual-tactile completion of objects based on embodied robots. (a) Shows the data collection process for both modalities. (b) shows a vision-only completion approach. (c) shows demonstrates the impact of the introduction of tactile data on completion.  </span>
            </div>
          </div>
        </div>
      </div>
    </div>

    <section class="img">
      <div style="display: flex; justify-content: center; align-items: center;">  
        <img src="./static/images/model.png" alt="Centered Image"/>  
      </div>
    </section>
    <section class="section3">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <div class="is-size-5 publication-authors">
                <span class="author-block">Fig3.  The concept-aware point cloud completion framework. The input data is divided equally  into  16 groups and extracted features from them for completion and classification. The tactile data that has been spatially calibrated can be directly spliced with visual data to co-optimize the target task. The refine operation reduces the effect of observational aberrations on the completion results. </span>
              </div>
            </div>
          </div>
        </div>
      </div>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text"><br><br>
              <strong>Method</strong><br><br>
                <strong> Group Feature Embedding</strong>
                We introduce a novel encoder to extract features named Group Feature Extraction Module(GCE).
                In order to extract sufficient local information, we divide the point clouds into  16 groups, which is capable of extracting the characteristics of object components.
                We first utilize the Farthest Point Sampling (FPS) to select the key points.  
                Then, we propose  using the Hilbert curve and the Trans-Hilbert using the Hilbert curve and the Trans-Hilbert curve to generate serialized key points. The KNN is used to form point patches. 
                It could generate serialized point tokens. The point clouds are divided into specified number of group with position encoding. 
                A mamba block combined multi-layer perception (MMLP) to direct point cloud classification. <br><br>
                <strong> Cognition-driven-guided Fusion Module </strong> 
                This module  processes the grouped point clouds by MMLP. Here, we design the fusion module to
                introduce the category feature into the model to guide the completion of the point cloud.  The cognition-driven-guided Fusion Module(CGF) processes the feature into four vectors. These vectors are pooled and concatenated together to form a composite latent vector (size = 1920). 
                Self-attention mechanism(SA) is widely used because it can capture the dependencies between different positions in a sequence. 
                Therefore, we introduce SA into the fusion module to extract the contextual information of the point cloud, thereby adequately capturing the links of point clouds is processed with SA.
                However, when the observational information about an object is residual, it can lead to similar characteristics of different kinds of objects. Model would treat it as other categories during the learning process. To address this limitation, we introduce the concept of categorical bootstrapping, giving the category information to the model early in the point cloud completion.
                This category guidance way makes the process of point cloud completion accurate and efficient.
               


            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text"><br><br>
              <strong>Experiment</strong><br><br>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/shapenet.png" alt="Centered Image"/>  
    </div>
  </section>    <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/visual_result.png" alt="Centered Image"/>  
    </div>
  </section>


   

  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 4: The experimental results on ShapeNet and VisTacPoint-V1. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
             *****
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="experiencevedio">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-10 publication-text">
              We collect two datasets : VisTacPoint-V1 and VisTacPoint-V2. The link is *********.com
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <video autoplay controls muted loop playsinline style="width: 25%; justify-content: center;">
        <source src="./static/images/tactile_cali.mp4" type="video/mp4">
      </video>
    </div>
  </section>


  <style>
    /* 确保所有文字居中 */
    .has-text-centered {
      text-align: center;
    }
   
    /* 去掉作者名字的超链接样式，仅保留文本 */
    .publication-authors .author-block a {
      text-decoration: none; /* 去掉下划线 */
      color: inherit;         /* 继承父元素颜色 */
      pointer-events: none;   /* 禁用鼠标事件，防止点击 */
      cursor: default;        /* 鼠标指针变为默认样式 */
    }
   
    /* 可选：优化链接按钮的显示，使整体看起来更整齐 */
    .publication-links .link-block {
      margin: 0 5px;
    }
  </style>
</body>
</html>
